# 大模型常用分词算法-Tokenizer



## 思路

一般来说，tokenizer有三种粒度：

### word
- 思路
    - 按照词进行分词
- 优点
    - 词的边界和含义得到保留
- 缺点
    - 1）词表大，稀有词学不好；
    - 2）OOV（可能超出词表外的词）；
    - 3）无法处理单词形态关系和词缀关系，会将两个本身意思一致的词分成两个毫不相同的ID，在英文中尤为明显，如：cat， cats。

### char
- 思路
    - 按照单字符进行分词，就是以char为最小粒度。
- 优点
    - 词表极小。比如：26个英文字母几乎可以组合出所有词，5000多个中文常用字基本也能组合出足够的词汇；
- 缺点
    - 1）无法承载丰富的语义，英文中尤为明显，但中文却是较为合理，中文中用此种方式较多；
    - 2）序列长度大幅增长。

### subword
- 思路
    - 按照词的subword进行分词。
- 优点
    - 较好的平衡词表大小与语义表达能力
- 算法
    - BPE
    - BBPE
    - WordPiece
    - SentencePiece



## BPE

### 定义
全称，Byte-Pair Encoding，用于将文本分解为子词或字符级别的单位。
### 流程
- 1.初始化词典
    - 将每个字符视为一个初始的词。
    - 并在每一个词最后，加上特殊字符：\</w>
    - 例如，对于输入文本"hello world"，初始词典可以包含{'h', 'e', 'l’, 'l', 'o’, '\</w>', 'w', 'r’, 'l', ‘d’, '\</w>'}。
- 2.统计词频
    - 对于每个词，统计其在文本中的频率。
    - 例如，在"hello world"中，'\</w>’出现2次，'h'出现1次，'e'出现1次，'l'出现3次，'o'出现2次，'w'出现1次，'r'出现1次，'d'出现1次。
- 3.合并频率最高的词对
    - 在每次迭代中，寻找频率最高的相邻词对，进行合并。
    - 合并的方式是将两个词连接起来。
    - 例如，在初始词典中，选择频率最高的词对"l"和"l"，将它们合并为"ll"，更新词典为{'h', 'e', 'll', 'o', 'w', 'r', 'd', '\</w>'}。
- 4.更新词频
    - 更新合并后的词频。
    - 对于合并的词，统计其在文本中的频率。
    - 频数为0的词，剔除出词表
    - 例如，在"hello world"中，'\</w>’出现2次，'h'出现1次，'e'出现1次，'ll’出现1次，'l’出现1次，'o'出现2次，'w'出现1次，'r'出现1次，'d'出现1次。
- 5.重复步骤3和4
    - 重复步骤3和4，直到达到预设的词典大小或者满足其他停止条件。
    - 每次迭代都会合并频率最高的词对，并更新词频。
### 使用模型
- GPT-2
- RoBERTa
- XLM
- FlauBERT


## BBPE
### 定义
- Byte-level BPE，从字节（byte）粒度进行分词（基于UTF8编码）
- byte包含了8 位bit，每个位(bit) 值为0或1，故1 byte包含了256种取值（2^8）
- 故初始词表包含了256个token
- 其他流程和BPE类似
### 流程
- 1.初始化词典
    - 包含一个字节的所有表示（256）。
- 2.统计词频
    - 对于每个词，统计其在文本中的频率。
- 3.合并频率最高的词对
    - 在每次迭代中，寻找频率最高的相邻词对，进行合并。
    - 合并的方式是将两个词连接起来。
- 4.更新词频
    - 更新合并后的词频。
    - 对于合并的词，统计其在文本中的频率。
    - 频数为0的词，剔除出词表
- 5.重复步骤3和4
    - 重复步骤3和4，直到达到预设的词典大小或者满足其他停止条件。
    - 每次迭代都会合并频率最高的词对，并更新词频。
### 缺点
    - 一个中文字符是由三个byte组成的，故llama初始词表包含的中文较少。
    - 使用llama中文版，一般都需要先扩充词表
### 使用模型
    - Llama


## WordPiece
### 定义
- 流程与BPE类似，除了合并子词的逻辑
- BPE选择频数最高的相邻子词合并
- WordPiece选择能够提升语言模型概率最大的相邻子词加入词表
    - 即具有最大的互信息值的两个子词
### 流程
- 1.初始化词典
    - 将每个字符视为一个初始的词。
- 2.统计词频
    - 对于每个词，统计其在文本中的频率。
- 3.合并收益最高的词对
    - 假设各个子词之间是独立存在的，则句子S的语言模型似然值等价于所有子词概率的乘积
<img src=https://github.com/wzzzd/LLM_Learning_Note/blob/main/img/model/tokenizer-wordpiece-func1.png width=30% />
    - 假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子S似然值的变化可表示为
<img src=https://github.com/wzzzd/LLM_Learning_Note/blob/main/img/model/tokenizer-wordpiece-func2.png width=35% />
    - 这里的语言模型可以选择最简单的Unigram-LM（一元分词语言模型），即单词的P等于其频数

- 4.更新词频
    - 更新合并后的词频。
    - 对于合并的词，统计其在文本中的频率。
    - 频数为0的词，剔除出词表
- 5.重复步骤3和4
    - 重复步骤3和4，直到达到预设的词典大小或者满足其他停止条件。
    - 每次迭代都会合并频率最高的词对，并更新词频。

### 使用模型
- Bert
- Electra
- DistilBERT





