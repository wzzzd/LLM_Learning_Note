# LLM_Learning_Note



# 介绍
用于记录Large Language Model相关的学习资料、内容，以及理解。

# 注意
如果使用Chrome浏览器，建议安装以下插件，并刷新页面
- [TeX All the Things](https://chrome.google.com/webstore/detail/tex-all-the-things/cbimabofgmfdkicghcadidpemeenbffn/related)
- [MathJax Plugin for Github](https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima)


# 模块
- Library Resource
    - Transformers
    - DeepSpeed
    - Megatron-LM
    - Jax
    - Colossal-AI
    - BMTrain
    - FastMoE
- Model
    - GPT series
        - [GPT-1](https://github.com/wzzzd/LLM_Learning_Note/blob/main/model/gpt-series/gpt-1.md)
        - [GPT-2](https://github.com/wzzzd/LLM_Learning_Note/blob/main/model/gpt-series/gpt-2.md)
        - [GPT-3](https://github.com/wzzzd/LLM_Learning_Note/blob/main/model/gpt-series/gpt-3.md)
        - [Codex](https://github.com/wzzzd/LLM_Learning_Note/blob/main/model/gpt-series/codex.md)
        - [InstructGPT/ChatGPT](https://github.com/wzzzd/LLM_Learning_Note/blob/main/model/gpt-series/instructgpt-chatgpt.md)
        - GPT-4
- Adaptation tuning of LLMs
    - Instuction tuning
        - [FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS](https://arxiv.org/pdf/2109.01652.pdf)
    - Alignment tuning
        -RLHF
            - Training language models to follow instructions with human feedback
            - Deep reinforcement learning from human preferences
    - [Efficient tuning](https://github.com/wzzzd/LLM_Learning_Note/blob/main/Tuning/efficient-tuning.md)
        - Adapter tuning
        - Prefix tuning
        - Prompt tuning
        - P-tuning
        - P-tuning v2
        - LoRA
- Utilization
    - [In-Context learning](https://github.com/wzzzd/LLM_Learning_Note/blob/main/Tuning/In-context-learning.md)
    - Chain-of-Thought Prompting
        - Chain of thought prompting elicits reasoning in large language models
- Capacity Evaluation
    - 




